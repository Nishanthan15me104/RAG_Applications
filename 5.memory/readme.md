# ğŸ§  Chat Memory Management Prototype with LangChain & Groq

This project demonstrates how to manage **short-term** and **long-term memory** in a conversational system using **LangChain**, **Groq LLM**, and **FAISS**.  
âš ï¸ It is **not a complete chatbot**â€”this is a memory management prototype meant for internal logic experimentation.

---

## ğŸ”§ Setup Components

- **LLM**: [`gemma2-9b-it`](https://groq.com) via `ChatGroq`.
- **Embeddings**: HuggingFace's `all-MiniLM-L6-v2`.
- **Environment Config**: Uses `python-dotenv` to securely load `GROQ_API_KEY`.

---

## ğŸ§¾ Long-Term Memory (FAISS)

- Stores persistent facts about the user using FAISS vector store.
- Used for retrieving background knowledge across different sessions.
- Example entries:
  - `"User's name is Nishanthan and prefers analogies."`
  - `"User has experience with RAG-based systems."`

You can store new interactions like:
```python
vectorstore.add_documents([
    Document(page_content=f"User: {user_input}"),
    Document(page_content=f"Assistant: {response}")
])

## ğŸ’¬ Short-Term Memory (ConversationSummaryBufferMemory)

Short-term memory is powered by LangChainâ€™s `ConversationSummaryBufferMemory`.

- ğŸ§  **Automatically condenses** old messages when they exceed the token limit (e.g., 1000 tokens).
- ğŸ”„ **Keeps recent context** alive for fluid conversation.
- âœ¨ **LLM-generated summaries** help retain coherence in long interactions.

```python
short_term_memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=1000,
    return_messages=True
)
### ğŸ§ª Conversation Simulation
User interactions are simulated using either a list of inputs or a function call:

```python
chat_with_bot(user_input, store_to_long_term=False)
```

This function:
- âœ… Generates a response using the `ConversationChain`.
- âœ… Optionally stores the user-input and LLM response to **long-term memory** using FAISS.
- âœ… Displays updated **short-term memory buffer** (summarized + recent).

---

## ğŸ§  Optional Features (Commented for Now)

### ğŸ” Long-Term Memory Retrieval
Retrieve user-specific context from the FAISS store:

```python
retrieved_docs = vectorstore.similarity_search("What do you know about the user?")
```

### ğŸ“š Conversation Summarization
Manually summarize the short-term conversation memory:

```python
summary_chain = load_summarize_chain(llm, chain_type="map_reduce")
summary = summary_chain.run([
    Document(page_content=msg.content) for msg in short_term_memory.chat_memory.messages
])
```

---

## ğŸ§µ Follow-up Interactions
Once the conversation starts, follow-up questions like:

```python
"Can you give me tips for solving ML case studies?"
"What should I focus on when answering business case questions in interviews?"
```

These will rely **only on short-term memory**, unless explicitly added to long-term memory by:

```python
chat_with_bot(user_input, store_to_long_term=True)
```
