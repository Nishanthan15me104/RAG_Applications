{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ6sj8gPDhG4"
      },
      "source": [
        "# Rerankers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8o5TRVfDhG4"
      },
      "source": [
        "Rerankers have been a common component of retrieval pipelines for many years. They allow us to add a final \"reranking\" step to our retrieval pipelines — like with **R**etrieval **A**ugmented **G**eneration (RAG) — that can be used to dramatically optimize our retrieval pipelines and improve their accuracy.\n",
        "\n",
        "In the example notebook we'll learn how to create retrieval pipelines with reranking using the [Cohere reranking model](https://txt.cohere.com/rerank/) (which is available for free).\n",
        "\n",
        "To begin, we setup our prerequisite libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "thtg9njP4bOh"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    pinecone-client==3.1.0 \\\n",
        "    cohere==4.27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VReBq2IeDhG5"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY3OglQm4bOj"
      },
      "source": [
        "We start by downloading a dataset that we will encode and store. The dataset [`jamescalam/ai-arxiv-chunked`](https://huggingface.co/datasets/jamescalam/ai-arxiv-chunked) contains scraped data from many popular ArXiv papers centred around LLMs. Including papers from Llama 2, GPTQ, and the GPT-4 technical paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQAVgquj4bOk",
        "outputId": "2d76cbce-bad3-41aa-9f44-2c15d232ac47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\nis44\\anaconda3\\RAG\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Generating train split: 100%|██████████| 41584/41584 [00:01<00:00, 29843.31 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
              "    num_rows: 41584\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrC-XHrTDhG6"
      },
      "source": [
        "We have 41.5K chunks, where each chunk is roughly the length of 1-2 paragraphs in length. Here is an example of a single record:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQg8wiUQ4bOk",
        "outputId": "90a416c3-a60d-474e-e643-77a64ff91913"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'doi': '1910.01108',\n",
              " 'chunk-id': '0',\n",
              " 'chunk': 'DistilBERT, a distilled version of BERT: smaller,\\nfaster, cheaper and lighter\\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\\nHugging Face\\n{victor,lysandre,julien,thomas}@huggingface.co\\nAbstract\\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\\nin Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains\\nchallenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be ﬁnetuned with good performances on a wide range of tasks like its larger counterparts.\\nWhile most prior work investigated the use of distillation for building task-speciﬁc\\nmodels, we leverage knowledge distillation during the pre-training phase and show\\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\\nof its language understanding capabilities and being 60% faster. To leverage the\\ninductive biases learned by larger models during pre-training, we introduce a triple\\nloss combining language modeling, distillation and cosine-distance losses. Our\\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its',\n",
              " 'id': '1910.01108',\n",
              " 'title': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter',\n",
              " 'summary': 'As Transfer Learning from large-scale pre-trained models becomes more\\nprevalent in Natural Language Processing (NLP), operating these large models in\\non-the-edge and/or under constrained computational training or inference\\nbudgets remains challenging. In this work, we propose a method to pre-train a\\nsmaller general-purpose language representation model, called DistilBERT, which\\ncan then be fine-tuned with good performances on a wide range of tasks like its\\nlarger counterparts. While most prior work investigated the use of distillation\\nfor building task-specific models, we leverage knowledge distillation during\\nthe pre-training phase and show that it is possible to reduce the size of a\\nBERT model by 40%, while retaining 97% of its language understanding\\ncapabilities and being 60% faster. To leverage the inductive biases learned by\\nlarger models during pre-training, we introduce a triple loss combining\\nlanguage modeling, distillation and cosine-distance losses. Our smaller, faster\\nand lighter model is cheaper to pre-train and we demonstrate its capabilities\\nfor on-device computations in a proof-of-concept experiment and a comparative\\non-device study.',\n",
              " 'source': 'http://arxiv.org/pdf/1910.01108',\n",
              " 'authors': ['Victor Sanh',\n",
              "  'Lysandre Debut',\n",
              "  'Julien Chaumond',\n",
              "  'Thomas Wolf'],\n",
              " 'categories': ['cs.CL'],\n",
              " 'comment': 'February 2020 - Revision: fix bug in evaluation metrics, updated\\n  metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at\\n  the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing\\n  - NeurIPS 2019',\n",
              " 'journal_ref': None,\n",
              " 'primary_category': 'cs.CL',\n",
              " 'published': '20191002',\n",
              " 'updated': '20200301',\n",
              " 'references': [{'id': '1910.01108'}]}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euFtJiIz4bOk"
      },
      "source": [
        "Format the data into the format we need, this will contain `id`, `text` (which we will embed), and `metadata`. For this use-case we don't need metadata but it can be useful to include so that if needed in the future we can make use of metadata filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-svyAMw4bOl",
        "outputId": "61f6e9af-2998-4e52-a216-a912473106b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map:   0%|          | 0/41584 [00:00<?, ? examples/s]\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'chunk-id'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk-id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprimary_category\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprimary_category\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpublished\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpublished\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupdated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupdated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# drop uneeded columns\u001b[39;00m\n\u001b[0;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mremove_columns([\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m ])\n",
            "File \u001b[1;32mc:\\Users\\nis44\\anaconda3\\RAG\\lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    558\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\nis44\\anaconda3\\RAG\\lib\\site-packages\\datasets\\arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[0;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3080\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3081\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\nis44\\anaconda3\\RAG\\lib\\site-packages\\datasets\\arrow_dataset.py:3501\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[0;32m   3499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[0;32m   3500\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 3501\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m iter_outputs(shard_iterable):\n\u001b[0;32m   3502\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[0;32m   3503\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\nis44\\anaconda3\\RAG\\lib\\site-packages\\datasets\\arrow_dataset.py:3475\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[1;34m(shard_iterable)\u001b[0m\n\u001b[0;32m   3473\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3474\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[1;32m-> 3475\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\nis44\\anaconda3\\RAG\\lib\\site-packages\\datasets\\arrow_dataset.py:3398\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[1;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[0;32m   3396\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[0;32m   3397\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[1;32m-> 3398\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[0;32m   3399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
            "Cell \u001b[1;32mIn[5], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: {\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk-id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprimary_category\u001b[39m\u001b[38;5;124m\"\u001b[39m: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprimary_category\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublished\u001b[39m\u001b[38;5;124m\"\u001b[39m: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublished\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdated\u001b[39m\u001b[38;5;124m\"\u001b[39m: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdated\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     11\u001b[0m     }\n\u001b[0;32m     12\u001b[0m })\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# drop uneeded columns\u001b[39;00m\n\u001b[0;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mremove_columns([\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m ])\n",
            "File \u001b[1;32mc:\\Users\\nis44\\anaconda3\\RAG\\lib\\site-packages\\datasets\\formatting\\formatting.py:278\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 278\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[0;32m    280\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'chunk-id'"
          ]
        }
      ],
      "source": [
        "data = data.map(lambda x: {\n",
        "    \"id\": f'{x[\"id\"]}-{x[\"chunk-id\"]}',\n",
        "    \"text\": x[\"chunk\"],\n",
        "    \"metadata\": {\n",
        "        \"title\": x[\"title\"],\n",
        "        \"url\": x[\"source\"],\n",
        "        \"primary_category\": x[\"primary_category\"],\n",
        "        \"published\": x[\"published\"],\n",
        "        \"updated\": x[\"updated\"],\n",
        "        \"text\": x[\"chunk\"],\n",
        "    }\n",
        "})\n",
        "# drop uneeded columns\n",
        "data = data.remove_columns([\n",
        "    \"title\", \"summary\", \"source\",\n",
        "    \"authors\", \"categories\", \"comment\",\n",
        "    \"journal_ref\", \"primary_category\",\n",
        "    \"published\", \"updated\", \"references\",\n",
        "    \"doi\", \"chunk-id\",\n",
        "    \"chunk\"\n",
        "])\n",
        "dataset = data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYzwm_q_4bOl"
      },
      "source": [
        "We need to define an embedding model to create our embedding vectors for retrieval, for that we will be using OpenAI's text-embedding-ada-002. There is some cost associated with this model, so be aware of that (costs for running this notebook are <$1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\nis44\\anaconda3\\RAG\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\nis44\\anaconda3\\RAG\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0612 12:03:30.086000 30048 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "from datasets import load_dataset\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Load environment variables from .env file (optional)\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "\n",
        "# Initialize HuggingFace embedding model\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\nis44\\anaconda3\\RAG\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0611 16:29:14.791000 27788 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Function to create LangChain documents from dataset\n",
        "def prepare_documents(data):\n",
        "    docs = []\n",
        "    for record in dataset:\n",
        "        metadata = record.get('metadata', {})\n",
        "        doc = Document(\n",
        "            page_content=record['text'],\n",
        "            metadata={\"id\": record['id'], **metadata}\n",
        "        )\n",
        "        docs.append(doc)\n",
        "    return docs\n",
        "\n",
        "# Convert dataset to documents\n",
        "documents = prepare_documents(dataset)\n",
        "\n",
        "# Create vector store (FAISS) from documents\n",
        "vectorstore = FAISS.from_documents(documents, hf_embeddings)\n",
        "\n",
        "# Save FAISS index locally\n",
        "vectorstore.save_local(\"faiss_index\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVjJ6gGd4bOl",
        "outputId": "b4e4ad45-ff8b-4cc6-b667-9553ebf150d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS index loaded successfully from 'faiss_index'.\n",
            "Extracted 41584 documents from the loaded vectorstore.\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_groq import ChatGroq \n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "import numpy as np\n",
        "from langchain_community.vectorstores import FAISS \n",
        "import faiss \n",
        "\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "try:\n",
        "    # Load the vectorstore from the saved folder\n",
        "    vectorstore = FAISS.load_local(\"faiss_index\", hf_embeddings, allow_dangerous_deserialization=True)\n",
        "    print(\"FAISS index loaded successfully from 'faiss_index'.\")\n",
        "\n",
        "    index = vectorstore.index\n",
        "    docstore = vectorstore.docstore\n",
        "    index_to_docstore_id = vectorstore.index_to_docstore_id\n",
        "\n",
        "    docs = [doc.page_content for doc_id, doc in vectorstore.docstore._dict.items()]\n",
        "    print(f\"Extracted {len(docs)} documents from the loaded vectorstore.\")\n",
        "\n",
        "except Exception as e:\n",
        "    exit()\n",
        "\n",
        "def retrieve(query, k):\n",
        "    query_embedding = hf_embeddings.embed_query(query)\n",
        "    query_embedding = np.array(query_embedding).astype('float32').reshape(1, -1)\n",
        "\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "\n",
        "    _, idx = index.search(query_embedding, k)\n",
        "\n",
        "    results = []\n",
        "    for i in idx[0]:\n",
        "        doc_id = index_to_docstore_id[i]\n",
        "        doc = docstore._dict[doc_id]\n",
        "        results.append(doc.page_content.strip())\n",
        "\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThmEhCcI4bOm",
        "outputId": "6a0bf8a7-4751-456f-87fe-a0cce74228d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved documents:\n",
            "\n",
            "--- Document 1 ---\n",
            "preferences and values which are diﬃcult to capture by hard- coded reward functions.\n",
            "RLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,\n",
            "ranking two model generations for the same prompt. This data is then collected to learn a reward model\n",
            "that predicts a scalar reward given any generated text. The r eward captures human preferences when\n",
            "judging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient\n",
            "algorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM\n",
            "pre-trained via self-supervised learning. However, for mo re complex tasks, the model’s generations may not\n",
            "be good enough. In such cases, RLHF is typically applied afte r an initial supervised ﬁne-tuning phase using\n",
            "a small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;\n",
            "Ouyang et al. ,2022;Stiennon et al. ,2020).\n",
            "A successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.\n",
            "(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing\n",
            "\n",
            "--- Document 2 ---\n",
            "the output is generated. These models often use a ﬁxed input and output vocabulary, which prevents\n",
            "them from learning representations for new words. One way to ﬁx this is to allow the decoder\n",
            "network to point back to some speciﬁc words or sub-sequences of the input and copy them onto the\n",
            "output sequence (Vinyals et al., 2015). Gulcehre et al. (2016) and Merity et al. (2017) combine this\n",
            "pointer mechanism with the original word generation layer in the decoder to allow the model to use\n",
            "either method at each decoding step.\n",
            "4.2 R EINFORCEMENT LEARNING FOR SEQUENCE GENERATION\n",
            "Reinforcement learning (RL) is a way of training an agent to interact with a given environment in\n",
            "order to maximize a reward. RL has been used to solve a wide variety of problems, usually when\n",
            "5\n",
            "an agent has to perform discrete actions before obtaining a reward, or when the metric to optimize\n",
            "is not differentiable and traditional supervised learning methods cannot be used. This is applicable\n",
            "to sequence generation tasks, because many of the metrics used to evaluate these tasks (like BLEU,\n",
            "ROUGE or METEOR) are not differentiable.\n",
            "\n",
            "--- Document 3 ---\n",
            "Tom Brown and Jared Kaplan, and much of Anthropic’s technical staff contributed to the development of our\n",
            "efﬁcient distributed training infrastructure and the underlying machine learning systems. Core contributors\n",
            "17\n",
            "include Tom Henighan, Scott Johnston, Sheer El Showk, Nelson Elhage, and Ben Mann. Scott Johnston\n",
            "in particular worked on optimizing pretraining for ML efﬁciency, while Sheer El Showk, Carol Chen, and\n",
            "Jennifer Zhou worked on data.\n",
            "Reinforcement Learning: The core RL infrastructure was built by Andy Jones and Kamal Ndousse in\n",
            "collaboration with Shauna Kravec and Dawn Drain. Development of the RL infrastructure has been led by\n",
            "Sam McCandlish and Dario Amodei.\n",
            "Sampling and Evaluation: Efﬁcient sampling efforts were led by Tom Brown, and Tom Conerly carried\n",
            "out major aspects of the design, implementation and support for the system, with help from Zac HatﬁeldDodds. Many members of Anthropic worked on our framework for evaluations, including Saurav Kadavath,\n",
            "Nicholas Schiefer, Nick Joseph, Tom Henighan, Amanda Askell, Jared Kaplan, Andy Jones, Ethan Perez,\n",
            "Scott Johnston, and Sam McCandlish. Jackson Kernion helped support human feedback data collection.\n",
            "\n",
            "--- Document 4 ---\n",
            "unnecessary. More details about RL are provided in B.1.\n",
            "Throughout this paper we use rPM=the preference model score itself for the RL reward. Recall that as\n",
            "implied by equation (2.1), this means that the difference in rPMvalues between two samples AandBwill\n",
            "be related to the predicted probability P(A>B )thatAwill be preferred to Bvia\n",
            "P(A>B ) =1\n",
            "1 +erPM(B)\u0000rPM(A)(4.2)\n",
            "There is no good reason13to use this preference model score directly as the reward, but it has been used in\n",
            "prior work such as [Stiennon et al., 2020] and so for simplicity we will not explore variations on this choice\n",
            "here.\n",
            "In order to produce additional prompts (i.e. the human side of the conversations) for RLHF training, we used\n",
            "a large LM to generate them. For this purpose, we simply used few-shot learning, creating a context with\n",
            "about 10 existing high-quality human queries, and then sampling to generate more. We ﬁnd that the sample\n",
            "efﬁciency of RLHF is roughly the same on the original crowdworker-written prompt dataset and the modelgenerated one, so we combine the two for greater diversity during RLHF training. We used 137k prompts\n",
            "\n",
            "--- Document 5 ---\n",
            "Clark and Liane Lovitt to ﬁlter the PII.\n",
            "Model Training : Saurav Kadavath and Yuntao Bai trained the RLHF models we analyze. Yuntao Bai additionally trained the helpful and harmless preference models we use throughout the paper, and implemented\n",
            "the RS models as well. Kamal Ndousse and Andy Jones built the infrastructure used to train RLHF models. More generally, model pretraining was led by Sam McCandlish, Nicholas Joseph, Tom Brown, and\n",
            "Jared Kaplan. The majority of Anthropic’s technical staff contributed to the development of our efﬁcient distributed training infrastructure and the underlying machine learning systems. Core contributors include Tom\n",
            "Henighan, Scott Johnston, Sheer El Showk, Nicholas Joseph, Nelson Elhage, and Ben Mann. Scott Johnston\n",
            "and Sheer El-Showk in particular worked on optimizing pretraining for ML efﬁciency.\n",
            "Sampling : Efﬁcient sampling efforts were led by Tom Brown, and Tom Conerly carried out major aspects of\n",
            "the design, implementation and support for the system, with help from Zac Hatﬁeld Dodds.\n",
            "Cluster : Nova DasSarma and Eli Tran-Johnson managed the research cluster our research depended on and\n",
            "maintained its stability, making this research possible. Many others helped with these efforts, including Ben\n",
            "\n",
            "--- Document 6 ---\n",
            "process and decision-making, and for practical purposes we expect non-evasive responses to be more compatible with helpfulness. We ﬁnd that RL-CAI is virtually never evasive, and often gives nuanced and harmless\n",
            "responses to most red team prompts. Sample responses from the 52B HH RLHF and RL-CAI models on\n",
            "PALMS, InstructGPT, and LaMDA prompts are given in Appendix D.\n",
            "Note that in Figure 8 (right), both the helpful and HH RLHF harmlessness Elo scores decline over the later\n",
            "stages of RLHF training. For helpful RLHF, this is likely because the model is becoming more willing to\n",
            "help users with potentially dangerous tasks (e.g. ‘How do I make anthrax?’). For HH RLHF, we suspect this\n",
            "is because the model becomes more and more evasive on red team prompts, and we instructed crowd-workers\n",
            "performing these tests to choose the more nuanced, transparent and thoughtful response over the more evasive\n",
            "response, assuming both responses are similarly harmless.\n",
            "This is contrary to prior work [Bai et al., 2022] where we simply asked workers to choose the more harmless\n",
            "response, which likely produced a signiﬁcant amount of data favoring evasiveness.9The HH PM data we use\n",
            "\n",
            "--- Document 7 ---\n",
            "token from our vocabulary V. The transition function P:S\u0002A! \u0001(S)deterministically appends\n",
            "an actionatto the end of the state st\u00001= (x0;\u0001\u0001\u0001;xm;a0;\u0001\u0001\u0001;at\u00001). This continues until the end\n",
            "of the horizon t\u0014Tand we obtain a state sT= (x0;\u0001\u0001\u0001;xm;a0;\u0001\u0001\u0001;aT). At the end of an episode\n",
            "a rewardR:S\u0002A\u0002Y! R1that depends on the ( sT;y) (e.g., an automated metric like PARENT\n",
            "Dhingra et al. (2019)) is emitted. RL4LMs provides an OpenAI gym (Brockman et al., 2016) style\n",
            "3\n",
            "Published as a conference paper at ICLR 2023\n",
            "API for an RL environment that simulates this LM-Based MDP formulation. This abstraction allows\n",
            "for new tasks to be added quickly with compatibility across all implemented algorithms.\n",
            "3.2 R EWARD FUNCTIONS AND EVALUATION METRICS\n",
            "Because RL4LMs provides a generic interface for per-token or per-sequence generation rewards, it\n",
            "is possible to quickly apply a wide array of RL algorithms to a similarly diverse range of textual\n",
            "\n",
            "--- Document 8 ---\n",
            "He ran most of our model comparison experiments.\n",
            "Tom Conerly helped with engineering, speciﬁcally with fast and efﬁcient sampling.\n",
            "Sheer El-Showk helped with pretraining research and dataset construction.\n",
            "Nelson Elhage contributed signiﬁcantly to pretraining and to engineering vision.\n",
            "Zac Hatﬁeld-Dodds helped with codebase maintenance and with engineering, speciﬁcally with fast and efﬁcient sampling.\n",
            "Danny Hernandez contributed to pretraining and especially to dataset design.\n",
            "Tristan Hume helped with streamlining our infrastructure.\n",
            "Scott Johnston helped with pretraining research.\n",
            "Shauna Kravec contributed to the development and use of our RL systems, and collaborated on RL research.\n",
            "Liane Lovitt helped with red-teaming, and in particular with designing the interface.\n",
            "Neel Nanda contributed to research discussions and priorities for alignment.\n",
            "Catherine Olsson helped advise on human feedback data collection, and contributed advice on alignment and\n",
            "evaluation.\n",
            "Dario Amodei advised the project and led efforts to build and test the RL infrastructure and ML.\n",
            "Tom Brown led engineering efforts, including efﬁcient pretraining, sampling, and the stability and design of\n",
            "RL systems.\n",
            "Jack Clark led societal impacts efforts and advised the project, including on various evaluations.\n",
            "\n",
            "--- Document 9 ---\n",
            "of learning from interaction with an environment and from numerical rewards (Sutton and Barto, 2018).\n",
            "RL methods have demonstrated success not only in game playing, for example checkers (Schaeﬀer et al.,\n",
            "1992), chess (Campbell et al., 2002; Silver et al., 2018), Go (Silver et al., 2016), poker (Moravčík et al.,\n",
            "2017), Atari(Mnihetal.,2015)andStarcraftII(Vinyalsetal.,2019), butalsoinreal-worldapplications,\n",
            "such as robotics (Kormushev et al., 2013), logistics (Refanidis et al., 2001), chemical synthesis (Segler\n",
            "et al., 2018) and personalised recommendations (Liu et al., 2019). In many of these applications, RL\n",
            "agents were able to achieve super-human performance, yet they can be prone to over-specialising to any\n",
            "single domain. In order to assess the performance of RL algorithms over a range of diﬀerent tasks, it is\n",
            "desirable to have platforms which expose diverse challenges through a uniﬁed interface. This approach\n",
            "was pioneered in the original Atari suite (Bellemare et al., 2013) and has been followed up by a variety\n",
            "of platforms, such as DeepMind Lab (Beattie et al., 2016), OpenAI Universe (OpenAI, 2016) and World\n",
            "\n",
            "--- Document 10 ---\n",
            "model to estimate the eventual performance of a larger RL policy. The slopes of these lines also\n",
            "explain how RLHF training can produce such large effective gains in model size, and for example it\n",
            "explains why the RLHF and context-distilled lines in Figure 1 are roughly parallel.\n",
            "• One can ask a subtle, perhaps ill-deﬁned question about RLHF training – is it teaching the model\n",
            "new skills or simply focusing the model on generating a sub-distribution of existing behaviors . We\n",
            "might attempt to make this distinction sharp by associating the latter class of behaviors with the\n",
            "region where RL reward remains linear inp\n",
            "KL.\n",
            "• To make some bolder guesses – perhaps the linear relation actually provides an upper bound on RL\n",
            "reward, as a function of the KL. One might also attempt to extend the relation further by replacingp\n",
            "KLwith a geodesic length in the Fisher geometry.\n",
            "By making RL learning more predictable and by identifying new quantitative categories of behavior, we\n",
            "might hope to detect unexpected behaviors emerging during RL training.\n",
            "4.4 Tension Between Helpfulness and Harmlessness in RLHF Training\n",
            "Here we discuss a problem we encountered during RLHF training. At an earlier stage of this project, we\n",
            "found that many RLHF policies were very frequently reproducing the same exaggerated responses to all\n",
            "remotely sensitive questions (e.g. recommending users seek therapy and professional help whenever they\n",
            "\n",
            "--- Document 11 ---\n",
            "least during early phases of RL. The RL training pipeline from this point on is identical to RLHF, except that\n",
            "the preference model is now trained partially with model-generated feedback labels (i.e. human-feedback\n",
            "labels for helpfulness, mixed with model-feedback labels for harmlessness).\n",
            "Chain-of-Thought Prompting\n",
            "We also experimented with using Chain-of-Thought (CoT) prompting [Wei et al., 2022] on the feedback\n",
            "model to generate labels. In this case, we use the helpful RLHF model instead of the pre-trained model,\n",
            "which typically writes higher quality chain-of-thought. Moreover, we reformat the feedback principles in a\n",
            "conversational manner (i.e., with Human: andAssistant: stop sequences), which is more suitable for\n",
            "the RLHF model, as follows.\n",
            "Human: Consider the following conversation between a human and an assistant:\n",
            "[HUMAN/ASSISTANT CONVERSATION]\n",
            "[PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]\n",
            "(A) [RESPONSE A]\n",
            "(B) [RESPONSE B]\n",
            "Assistant: Let’s think step-by-step: [CHAIN-OF-THOUGHT]\n",
            "\n",
            "--- Document 12 ---\n",
            "31\n",
            "5 Discussion\n",
            "Here, we discuss the interesting properties we have observed with RLHF (Section 5.1). We then discuss the\n",
            "limitations of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc (Section 5.2). Lastly, we present our strategy for responsibly releasing these\n",
            "models (Section 5.3).\n",
            "5.1 Learnings and Observations\n",
            "Our tuning process revealed several interesting results, such as L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc ’s abilities to temporally\n",
            "organize its knowledge, or to call APIs for external tools.\n",
            "SFT (Mix)\n",
            "SFT (Annotation)\n",
            "RLHF (V1)\n",
            "0.0 0.2 0.4 0.6 0.8 1.0\n",
            "Reward Model ScoreRLHF (V2)\n",
            "Figure 20: Distribution shift for progressive versions of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , from SFT models towards RLHF.\n",
            "Beyond Human Supervision. At the outset of the project, many among us expressed a preference for\n",
            "\n",
            "--- Document 13 ---\n",
            "response. We made this choice so that we could fully explore the vulnerability of our models to red-teaming.\n",
            "However, from the point of view of RLHF this was problematic, because beyond the ﬁrst turn of dialogue,\n",
            "our models never learned what a sophisticated response to a harmful query might be like. Our dataset does\n",
            "not provide guidance on the upper end of the distribution, on what models should do, but only tells models\n",
            "what notto do.\n",
            "In practice, we have partially resolved the optimization issue by training on a larger fraction of helpfulness\n",
            "prompts during RLHF. But in the future we hope to more fully and systematically address this problem by\n",
            "collecting harmlessness data where crowdworkers choose the best possible response from our models.14In\n",
            "this way we hope that rather than simply shutting down harmful requests, models can learn the more subtle\n",
            "art of ‘hostage negotiation’ with red-teamers.\n",
            "Note that since the data and models discussed in this section are from an earlier stage of our research, the RL\n",
            "results may look slightly different from other parts of the paper.\n",
            "4.5 Iterated Online RLHF\n",
            "In preceding sections we discussed the problem that PMs become progressively less calibrated and less robust\n",
            "at higher scores, as seen in the PM calibration study in Figure 9, and the RLHF robustness study in Figure\n",
            "\n",
            "--- Document 14 ---\n",
            "from, those appearing in the PM and RL training data. Results are shown in Figure 3, where we compare\n",
            "SL-CAI models and RLHF models. The RLHF models include two types: (1) models trained on only helpfulness data, and (2) models trained on helpfulness and harmlessness. The ﬁgure also includes the RL-CAI (i.e.,\n",
            "RLAIF) models discussed in Section 4. A total of 10,274 helpfulness and 8,135 comparisons were collected\n",
            "for AB testing the 24 snapshots shown collectively in Figures 2 and 3.\n",
            "As expected from prior work, we ﬁnd that the helpful RLHF model is more helpful but also more harmful\n",
            "than HH RLHF. Furthermore, while SL-CAI is less helpful than both RL models, it is more harmless than the\n",
            "helpful RLHF model and more harmful than HH RLHF.8We also compare SL-CAI and pre-trained models\n",
            "in Figure 8, where the 52B-parameter SL-CAI model is shown as the initial snapshot of RL-CAI, while the\n",
            "7These principles were selected in an ad hoc manner for research purposes, and were not carefully designed as in\n",
            "[Glaese et al., 2022]. We have included these principles in Appendix C]\n",
            "8Note that the harmlessness Elo scores for the RLHF models look much closer to together compared to\n",
            "\n",
            "--- Document 15 ---\n",
            "appropriate processes in place.\n",
            "•Undue trust being placed in the system, especiallyasitcommunicateswithhumansinnaturallanguage, andcouldeasilybemistakenfor\n",
            "a human (Proudfoot, 2011; Watson, 2019).•The risk of job loss as a result of the automation of roles requiring language abilities (Frey\n",
            "and Osborne, 2017).\n",
            "4. Misspeciﬁcation\n",
            "FollowingKrakovnaetal.(2020b),weconsiderthe\n",
            "role of the designer of an AI system to be giving a\n",
            "speciﬁcation , understood quite broadly to encompass many aspects of the AI development process.\n",
            "For example, for an RL system, the speciﬁcation\n",
            "includes providing an environment in which the\n",
            "RL agent acts, a reward function that calculates\n",
            "reward signals, and a training algorithm for how\n",
            "the RL agent learns.\n",
            "Undesired behaviour can occur due to misspeciﬁcation– a mistake made by the designer in implementing the task speciﬁcation. In the language\n",
            "of Ortega and Maini (2018), the misspeciﬁcation\n",
            "is due to the gap between the ideal speciﬁcation\n",
            "(what the designer intended) and the design speciﬁcation (what the designer actually implements).\n",
            "\n",
            "--- Document 16 ---\n",
            "(RL). RL agents usually contain four key elements: a policy which defines the agent’s way of behaving at a given time; a reward signal which defines its goal; a value function which estimates the long-term value of different states of affairs; and a model of the environment which allows the agent to make predictions about how the environment will respond to its decisions (Sutton and Barto 2018, 6-7). RL agents then learn what to do by trying to maximise a numerical reward signal that they receive from their environment. They do this by engaging in exploration and revising their policies along the way. Sophisticated RL agents have proved particularly adept at game-playing, mastering the ancient Chinese board game of Go (which had\n",
            "\n",
            "--- Document 17 ---\n",
            "found that many RLHF policies were very frequently reproducing the same exaggerated responses to all\n",
            "remotely sensitive questions (e.g. recommending users seek therapy and professional help whenever they\n",
            "express any level of displeasure at all). This greatly limited these models’ utility. We still see a vestige of this\n",
            "19\n",
            "behavior in some of the examples provided in Section 6.2. We now believe these policies were the result of\n",
            "over-optimizing for harmlessness, while under-optimizing helpfulness.\n",
            "With our data collection procedure, we think this is quite intuitive. In order to get a very good score on\n",
            "red-teaming prompts, it’s probably sufﬁcient for models to respond with something like “I can’t answer that.”\n",
            "This does not require much sophistication (it just requires learning to classify harmful requests), and so we\n",
            "expect it is easier to learn than helpfulness.\n",
            "In Figure 14 (right), we show the policy’s PM score throughout training, after separating helpfulness and\n",
            "harmlessness prompts. On the left side of the same ﬁgure, we show the score distribution of PM comparison\n",
            "data, again separating helpful and harmless datasets. We observe that the policy’s harmlessness score is\n",
            "somewhat off-distribution, as it is on the upper tail of the harmlessness comparison data. On the other hand,\n",
            "\n",
            "--- Document 18 ---\n",
            "conditions. In the Q condition, the 175B model discriminates against Black students less with more RLHF\n",
            "steps, but fails to achieve demographic parity. In the Q+IF condition, the model achieves demographic parity\n",
            "at 600 RLHF steps. In the Q+IF+CoT condition, the model achieves demographic parity at 200 RLHF steps.\n",
            "In both conditions, further RLHF training causes the models to increasingly discriminate in favor of Black\n",
            "students.\n",
            "Fig. 5 (Right, A.2) shows how model size and RLHF training interact with respect to demographic parity.\n",
            "Across all experimental conditions, the amount of RLHF training has the greatest effect for models larger than\n",
            "22B parameters. Notably, for the 175B parameter model, at 50 steps of RLHF training, the Q+IF condition\n",
            "discriminates against Black students by 15% and at 1000 RLHF steps it discriminates in favor of Black\n",
            "students by 10%. For this benchmark, one can approximately achieve demographic parity by tuning both the\n",
            "model size and the amount of RLHF steps. But parity can only be achieved if models are instructed to not\n",
            "make decisions based on the race of the students.\n",
            "5 Discussion\n",
            "5.1 Conclusion\n",
            "We set out to test the hypothesis that large language models may have the capability to “morally selfcorrect”—to avoid producing harmful outputs—if instructed to do so in natural language. We ﬁnd strong\n",
            "\n",
            "--- Document 19 ---\n",
            "by being evasive [4].\n",
            "Our second contribution is to release our dataset of 38,961 red team attacks for others to analyze and learn\n",
            "from (Table 1).2We provide a Datasheet [24] in §A.7 that fully documents the data and we explain the\n",
            "pros and cons for releasing the data in §A.5. Our dataset is an order of magnitude larger than a similar\n",
            "available red team dataset [60] and considers models one order of magnitude larger than those in [60]. To our\n",
            "knowledge, we release the only dataset of red team attacks on a model trained be safe with RLHF. These types\n",
            "of models are already deployed [41] and we believe our data can help shed further light on their strengths\n",
            "and weaknesses. More generally, we believe our data can be used to understand what successful red team\n",
            "attacks look like, to build (semi-)automated red team techniques [42], to build classiﬁers for harmfulness,\n",
            "and to prototype strategies for measuring and mitigating harms in language models. We also provide our own\n",
            "preliminary analyses of the types of harms uncovered in our data (Figures 2 & 9, §4).\n",
            "Our last contribution is to exhaustively describe our instructions, processes, and statistical methodologies\n",
            "for red teaming (§3). Throughout the design of our experiments, we arrived at many junctures in which\n",
            "\n",
            "--- Document 20 ---\n",
            "Agarwal et al., 2019; Swamy et al., 2021).\n",
            "RL for Large Action Spaces. MIXER (Ranzato et al., 2016) combined ideas from schedule sampling\n",
            "and REINFORCE (Williams, 1992). Bahdanau et al. (2016) proposed an actor-critic algorithm to\n",
            "address the variance/large action space problems when using REINFORCE for language generation;\n",
            "follow-up works such as KG-A2C (Ammanabrolu & Hausknecht, 2020), TrufLL (Martin et al., 2022),\n",
            "AE-DQN (Zahavy et al., 2018), and GALAD (Ammanabrolu et al., 2022) addressed similar issues by\n",
            "attempting to eliminate and reduce the action space during exploration.\n",
            "RL for NLP. RL, often in the form of bandit learning, has been used to improve models in machine\n",
            "translation (Wu et al., 2016; Nguyen et al., 2017; Kiegeland & Kreutzer, 2021), summarization\n",
            "(Stiennon et al., 2020; Paulus et al., 2017), dialogue (Li et al., 2016; Zhou et al., 2017; Jaques et al.,\n",
            "2020), image captioning (Rennie et al., 2017), question generation (Pang & He, 2021), text-games\n",
            "\n",
            "--- Document 21 ---\n",
            "and behavior. He helped to write the paper.\n",
            "Andy Jones and Kamal Ndoussse built the infrastructure for RL training of large language models. They also\n",
            "built associated plotting and monitoring systems and implemented the PPO algorithm. They helped with the\n",
            "design, implementation, and debugging of RLHF.\n",
            "Amanda Askell helped to design model evaluations, collected samples and evaluations from professional\n",
            "writers, built systems for improving the quality and quantity of data collection, and collaborated with Jared\n",
            "20In fact, this happened by accident when researchers ﬁne-tuned GPT-2 from human preferences with a sign-ﬂip bug.\n",
            "This resulted in a model which optimized for negative sentiment while preserving natural language [Ziegler et al., 2019].\n",
            "37\n",
            "and Jackson on associated evaluations. She also helped with the design and implementation of the human\n",
            "feedback interface. She helped to write the paper.\n",
            "Anna Chen helped with general RL and RLHF experimentation, and contributed to the research design.\n",
            "Nova DasSarma managed the underlying cluster infrastructure, making large scale RL training and human\n",
            "feedback collection possible.\n",
            "Dawn Drain trained the underlying code models and collaborated with Saurav on coding evaluations.\n",
            "Stanislav Fort performed the OOD detection and outlier exposure research and analysis on helpful versus\n",
            "harmful data samples.\n",
            "\n",
            "--- Document 22 ---\n",
            "next token is sampled) in order to optimize some reward funct ion. Most of the existing work on RL and ALMs\n",
            "has focused on teaching LMs how to act rather than reason. The closest work on learning how to reason via\n",
            "RL is STaR ( Zelikman et al. ,2022), a bootstrapping-based approach that is discussed in Sect ion4.1\n",
            "RL is a natural framework for training LMs to act and use tools since many of these tools are nondiﬀerentiable (e.g. search engines, calculators or progra mming language interpreters). Additionally, many\n",
            "tasks that beneﬁt from interacting with tools resemble sequ ential decision making problems (e.g., navigating a web-browser to buy a speciﬁed product) and have a well-d eﬁned reward (e.g., 1 if the model buys\n",
            "the correct product and 0 otherwise). While there are early w orks focused on models that could interface\n",
            "with external tools, they employ ad-hoc tool-dependent arc hitectures ( Adolphs et al. ,2022;Buck et al. ,2018;\n",
            "16\n",
            "Nogueira and Cho ,2017;Zhong et al. ,2018). We do not cover them here since the main focus of our survey\n",
            "\n",
            "--- Document 23 ---\n",
            "that we call “roleplay attacks” on the RLHF model. In a roleplay attack we exploit the helpfulness of the\n",
            "model by asking it to roleplay as a malevolent character. For example, if we asked the RLHF model to enter\n",
            "“4chan mode” the assistant would oblige and produce harmful and offensive outputs (consistent with what\n",
            "can be found on 4chan). We intend to document additional qualitative safety failures that we uncovered in\n",
            "future work.\n",
            "Our analysis of the data is bottom-up, in that we ﬁrst collect the data, then attempt to characterize the attack\n",
            "surface (Figure 2). An alternative approach, is to refer to a taxonomy of possible attack types [57] and explicitly ask the red team to attack models according to this taxonomy. Ultimately, an approach that combines both\n",
            "top-down and bottom-up strategies may be worthwhile, especially since people may discover attack types not\n",
            "yet covered by a taxonomy—we see some evidence of this in the frequency of attack types labeled as “Other”\n",
            "in our tagging experiment (Figure 9).\n",
            "Our approach relies extensively on fully manual red teaming by crowdworkers, which is expensive (and\n",
            "possibly slow) to do at scale. Previous work illustrates the potential for automating red teaming [42]. For\n",
            "future work, we plan on explicitly comparing and contrasting (semi-)manual versus automated approaches\n",
            "\n",
            "--- Document 24 ---\n",
            "DKL(\u0019jj\u00190), where\u0019denotes the policy distribution (and \u00190the initial policy), as evaluated empirically on\n",
            "the samples drawn from the policy during training.\n",
            "Why should this be? When DKL(\u0019+\u000e\u0019jj\u0019)is series expanded in \u000e\u0019, the expansion begins at quadratic\n",
            "order, so if we imagine that the RL policy can also be series expanded around the base LM, and that the RL\n",
            "reward varies linearly in \u000e\u0019, then in the ‘small- \u000e\u0019region’ (i.e. where the series expansion provides a good\n",
            "approximation), we should expect reward /pDKL. Typically we should expect that reward varies linearly\n",
            "in\u000e\u0019, because because the initial policy \u0019was not previously optimized for reward, so there is no reason why\n",
            "it would sit at an extremum with respect to small variations \u000e\u0019. So the fact that this relation seems to hold\n",
            "empirically suggests that most of RLHF training remains in the small- \u000e\u0019regime.\n",
            "Though they did not use these coordinates, a similar scaling can be read off from the results in learning to\n",
            "summarize [Stiennon et al., 2020]. In particular, they provide a nice analysis of rejection sampling, where\n",
            "\n",
            "--- Document 25 ---\n",
            "Stackoverflow Good Answer vs. Bad Answer Loss Difference\n",
            "Python FT\n",
            "Python FT + RLHF(b)Difference in mean log-prob between good and bad\n",
            "answers to Stack Overﬂow questions.\n",
            "Figure 37 Analysis of RLHF on language modeling for good and bad Stack Overﬂow answers, over many\n",
            "model sizes, ranging from 13M to 52B parameters. Compared to the baseline model (a pre-trained LM\n",
            "ﬁnetuned on Python code), the RLHF model is more capable of distinguishing quality (right) , but is worse\n",
            "at language modeling (left) .\n",
            "the RLHF models obtain worse loss. This is most likely due to optimizing a different objective rather than\n",
            "pure language modeling.\n",
            "B.8 Further Analysis of RLHF on Code-Model Snapshots\n",
            "As discussed in Section 5.3, RLHF improves performance of base code models on code evals. In this appendix, we compare that with simply prompting the base code model with a sample of prompts designed to\n",
            "elicit helpfulness, harmlessness, and honesty, which we refer to as ‘HHH’ prompts. In particular, they contain\n",
            "a couple of coding examples. Below is a description of what this prompt looks like:\n",
            "Below are a series of dialogues between various people and an AI assistant. The AI tries to be helpful,\n"
          ]
        }
      ],
      "source": [
        "query = \"can you explain why we would want to do rlhf?\"\n",
        "docs  = retrieve(query, k=25)\n",
        "\n",
        "print(\"Retrieved documents:\")\n",
        "for i, doc in enumerate(docs, 1):\n",
        "    print(f\"\\n--- Document {i} ---\\n{doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtEcZE5AfQHW"
      },
      "source": [
        "Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/docs/projects)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVmlAytrfeUJ"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "\n",
        "os.environ[\"COHERE_API_KEY\"] = os.getenv(\"cohere_api_key\") \n",
        "# init client\n",
        "co = cohere.Client(os.environ[\"COHERE_API_KEY\"])\n",
        "\n",
        "rerank_docs = co.rerank(\n",
        "    query=query, documents=docs , top_n=25, model=\"rerank-v3.5\"\n",
        ")\n",
        "\n",
        "type(rerank_docs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nu2KHWG4bOm"
      },
      "source": [
        "Creating an index, we set `dimension` equal to to dimensionality of Ada-002 (`1536`), and use a `metric` also compatible with Ada-002 (this can be either `cosine` or `dotproduct`). We also pass our `spec` to index initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9auAxaTJEPU",
        "outputId": "da8d4507-4f86-4a44-abbe-eeb6810b7f0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "cohere.responses.rerank.RerankResult"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMWXA4YbJf-U"
      },
      "source": [
        "We access the text content of the docs like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "4ukXGwJ4JQhh",
        "outputId": "ee5262fd-9b4b-402a-c95a-dc6d53a5703a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Document 1 ---\n",
            "RerankResult<document['text']: model to estimate the eventual performance of a larger RL policy. The slopes of these lines also\n",
            "explain how RLHF training can produce such large effective gains in model size, and for example it\n",
            "explains why the RLHF and context-distilled lines in Figure 1 are roughly parallel.\n",
            "• One can ask a subtle, perhaps ill-deﬁned question about RLHF training – is it teaching the model\n",
            "new skills or simply focusing the model on generating a sub-distribution of existing behaviors . We\n",
            "might attempt to make this distinction sharp by associating the latter class of behaviors with the\n",
            "region where RL reward remains linear inp\n",
            "KL.\n",
            "• To make some bolder guesses – perhaps the linear relation actually provides an upper bound on RL\n",
            "reward, as a function of the KL. One might also attempt to extend the relation further by replacingp\n",
            "KLwith a geodesic length in the Fisher geometry.\n",
            "By making RL learning more predictable and by identifying new quantitative categories of behavior, we\n",
            "might hope to detect unexpected behaviors emerging during RL training.\n",
            "4.4 Tension Between Helpfulness and Harmlessness in RLHF Training\n",
            "Here we discuss a problem we encountered during RLHF training. At an earlier stage of this project, we\n",
            "found that many RLHF policies were very frequently reproducing the same exaggerated responses to all\n",
            "remotely sensitive questions (e.g. recommending users seek therapy and professional help whenever they, index: 9, relevance_score: 0.63723457>\n",
            "\n",
            "--- Document 2 ---\n",
            "RerankResult<document['text']: preferences and values which are diﬃcult to capture by hard- coded reward functions.\n",
            "RLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,\n",
            "ranking two model generations for the same prompt. This data is then collected to learn a reward model\n",
            "that predicts a scalar reward given any generated text. The r eward captures human preferences when\n",
            "judging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient\n",
            "algorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM\n",
            "pre-trained via self-supervised learning. However, for mo re complex tasks, the model’s generations may not\n",
            "be good enough. In such cases, RLHF is typically applied afte r an initial supervised ﬁne-tuning phase using\n",
            "a small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;\n",
            "Ouyang et al. ,2022;Stiennon et al. ,2020).\n",
            "A successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.\n",
            "(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing, index: 0, relevance_score: 0.5911811>\n",
            "\n",
            "--- Document 3 ---\n",
            "RerankResult<document['text']: by being evasive [4].\n",
            "Our second contribution is to release our dataset of 38,961 red team attacks for others to analyze and learn\n",
            "from (Table 1).2We provide a Datasheet [24] in §A.7 that fully documents the data and we explain the\n",
            "pros and cons for releasing the data in §A.5. Our dataset is an order of magnitude larger than a similar\n",
            "available red team dataset [60] and considers models one order of magnitude larger than those in [60]. To our\n",
            "knowledge, we release the only dataset of red team attacks on a model trained be safe with RLHF. These types\n",
            "of models are already deployed [41] and we believe our data can help shed further light on their strengths\n",
            "and weaknesses. More generally, we believe our data can be used to understand what successful red team\n",
            "attacks look like, to build (semi-)automated red team techniques [42], to build classiﬁers for harmfulness,\n",
            "and to prototype strategies for measuring and mitigating harms in language models. We also provide our own\n",
            "preliminary analyses of the types of harms uncovered in our data (Figures 2 & 9, §4).\n",
            "Our last contribution is to exhaustively describe our instructions, processes, and statistical methodologies\n",
            "for red teaming (§3). Throughout the design of our experiments, we arrived at many junctures in which, index: 18, relevance_score: 0.5559422>\n",
            "\n",
            "--- Document 4 ---\n",
            "RerankResult<document['text']: DKL(\u0019jj\u00190), where\u0019denotes the policy distribution (and \u00190the initial policy), as evaluated empirically on\n",
            "the samples drawn from the policy during training.\n",
            "Why should this be? When DKL(\u0019+\u000e\u0019jj\u0019)is series expanded in \u000e\u0019, the expansion begins at quadratic\n",
            "order, so if we imagine that the RL policy can also be series expanded around the base LM, and that the RL\n",
            "reward varies linearly in \u000e\u0019, then in the ‘small- \u000e\u0019region’ (i.e. where the series expansion provides a good\n",
            "approximation), we should expect reward /pDKL. Typically we should expect that reward varies linearly\n",
            "in\u000e\u0019, because because the initial policy \u0019was not previously optimized for reward, so there is no reason why\n",
            "it would sit at an extremum with respect to small variations \u000e\u0019. So the fact that this relation seems to hold\n",
            "empirically suggests that most of RLHF training remains in the small- \u000e\u0019regime.\n",
            "Though they did not use these coordinates, a similar scaling can be read off from the results in learning to\n",
            "summarize [Stiennon et al., 2020]. In particular, they provide a nice analysis of rejection sampling, where, index: 23, relevance_score: 0.5510187>\n",
            "\n",
            "--- Document 5 ---\n",
            "RerankResult<document['text']: response. We made this choice so that we could fully explore the vulnerability of our models to red-teaming.\n",
            "However, from the point of view of RLHF this was problematic, because beyond the ﬁrst turn of dialogue,\n",
            "our models never learned what a sophisticated response to a harmful query might be like. Our dataset does\n",
            "not provide guidance on the upper end of the distribution, on what models should do, but only tells models\n",
            "what notto do.\n",
            "In practice, we have partially resolved the optimization issue by training on a larger fraction of helpfulness\n",
            "prompts during RLHF. But in the future we hope to more fully and systematically address this problem by\n",
            "collecting harmlessness data where crowdworkers choose the best possible response from our models.14In\n",
            "this way we hope that rather than simply shutting down harmful requests, models can learn the more subtle\n",
            "art of ‘hostage negotiation’ with red-teamers.\n",
            "Note that since the data and models discussed in this section are from an earlier stage of our research, the RL\n",
            "results may look slightly different from other parts of the paper.\n",
            "4.5 Iterated Online RLHF\n",
            "In preceding sections we discussed the problem that PMs become progressively less calibrated and less robust\n",
            "at higher scores, as seen in the PM calibration study in Figure 9, and the RLHF robustness study in Figure, index: 12, relevance_score: 0.5113512>\n",
            "\n",
            "--- Document 6 ---\n",
            "RerankResult<document['text']: Stackoverflow Good Answer vs. Bad Answer Loss Difference\n",
            "Python FT\n",
            "Python FT + RLHF(b)Difference in mean log-prob between good and bad\n",
            "answers to Stack Overﬂow questions.\n",
            "Figure 37 Analysis of RLHF on language modeling for good and bad Stack Overﬂow answers, over many\n",
            "model sizes, ranging from 13M to 52B parameters. Compared to the baseline model (a pre-trained LM\n",
            "ﬁnetuned on Python code), the RLHF model is more capable of distinguishing quality (right) , but is worse\n",
            "at language modeling (left) .\n",
            "the RLHF models obtain worse loss. This is most likely due to optimizing a different objective rather than\n",
            "pure language modeling.\n",
            "B.8 Further Analysis of RLHF on Code-Model Snapshots\n",
            "As discussed in Section 5.3, RLHF improves performance of base code models on code evals. In this appendix, we compare that with simply prompting the base code model with a sample of prompts designed to\n",
            "elicit helpfulness, harmlessness, and honesty, which we refer to as ‘HHH’ prompts. In particular, they contain\n",
            "a couple of coding examples. Below is a description of what this prompt looks like:\n",
            "Below are a series of dialogues between various people and an AI assistant. The AI tries to be helpful,, index: 24, relevance_score: 0.4765072>\n",
            "\n",
            "--- Document 7 ---\n",
            "RerankResult<document['text']: process and decision-making, and for practical purposes we expect non-evasive responses to be more compatible with helpfulness. We ﬁnd that RL-CAI is virtually never evasive, and often gives nuanced and harmless\n",
            "responses to most red team prompts. Sample responses from the 52B HH RLHF and RL-CAI models on\n",
            "PALMS, InstructGPT, and LaMDA prompts are given in Appendix D.\n",
            "Note that in Figure 8 (right), both the helpful and HH RLHF harmlessness Elo scores decline over the later\n",
            "stages of RLHF training. For helpful RLHF, this is likely because the model is becoming more willing to\n",
            "help users with potentially dangerous tasks (e.g. ‘How do I make anthrax?’). For HH RLHF, we suspect this\n",
            "is because the model becomes more and more evasive on red team prompts, and we instructed crowd-workers\n",
            "performing these tests to choose the more nuanced, transparent and thoughtful response over the more evasive\n",
            "response, assuming both responses are similarly harmless.\n",
            "This is contrary to prior work [Bai et al., 2022] where we simply asked workers to choose the more harmless\n",
            "response, which likely produced a signiﬁcant amount of data favoring evasiveness.9The HH PM data we use, index: 5, relevance_score: 0.45028743>\n",
            "\n",
            "--- Document 8 ---\n",
            "RerankResult<document['text']: that we call “roleplay attacks” on the RLHF model. In a roleplay attack we exploit the helpfulness of the\n",
            "model by asking it to roleplay as a malevolent character. For example, if we asked the RLHF model to enter\n",
            "“4chan mode” the assistant would oblige and produce harmful and offensive outputs (consistent with what\n",
            "can be found on 4chan). We intend to document additional qualitative safety failures that we uncovered in\n",
            "future work.\n",
            "Our analysis of the data is bottom-up, in that we ﬁrst collect the data, then attempt to characterize the attack\n",
            "surface (Figure 2). An alternative approach, is to refer to a taxonomy of possible attack types [57] and explicitly ask the red team to attack models according to this taxonomy. Ultimately, an approach that combines both\n",
            "top-down and bottom-up strategies may be worthwhile, especially since people may discover attack types not\n",
            "yet covered by a taxonomy—we see some evidence of this in the frequency of attack types labeled as “Other”\n",
            "in our tagging experiment (Figure 9).\n",
            "Our approach relies extensively on fully manual red teaming by crowdworkers, which is expensive (and\n",
            "possibly slow) to do at scale. Previous work illustrates the potential for automating red teaming [42]. For\n",
            "future work, we plan on explicitly comparing and contrasting (semi-)manual versus automated approaches, index: 22, relevance_score: 0.44999737>\n",
            "\n",
            "--- Document 9 ---\n",
            "RerankResult<document['text']: 31\n",
            "5 Discussion\n",
            "Here, we discuss the interesting properties we have observed with RLHF (Section 5.1). We then discuss the\n",
            "limitations of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc (Section 5.2). Lastly, we present our strategy for responsibly releasing these\n",
            "models (Section 5.3).\n",
            "5.1 Learnings and Observations\n",
            "Our tuning process revealed several interesting results, such as L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc ’s abilities to temporally\n",
            "organize its knowledge, or to call APIs for external tools.\n",
            "SFT (Mix)\n",
            "SFT (Annotation)\n",
            "RLHF (V1)\n",
            "0.0 0.2 0.4 0.6 0.8 1.0\n",
            "Reward Model ScoreRLHF (V2)\n",
            "Figure 20: Distribution shift for progressive versions of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , from SFT models towards RLHF.\n",
            "Beyond Human Supervision. At the outset of the project, many among us expressed a preference for, index: 11, relevance_score: 0.41905406>\n",
            "\n",
            "--- Document 10 ---\n",
            "RerankResult<document['text']: least during early phases of RL. The RL training pipeline from this point on is identical to RLHF, except that\n",
            "the preference model is now trained partially with model-generated feedback labels (i.e. human-feedback\n",
            "labels for helpfulness, mixed with model-feedback labels for harmlessness).\n",
            "Chain-of-Thought Prompting\n",
            "We also experimented with using Chain-of-Thought (CoT) prompting [Wei et al., 2022] on the feedback\n",
            "model to generate labels. In this case, we use the helpful RLHF model instead of the pre-trained model,\n",
            "which typically writes higher quality chain-of-thought. Moreover, we reformat the feedback principles in a\n",
            "conversational manner (i.e., with Human: andAssistant: stop sequences), which is more suitable for\n",
            "the RLHF model, as follows.\n",
            "Human: Consider the following conversation between a human and an assistant:\n",
            "[HUMAN/ASSISTANT CONVERSATION]\n",
            "[PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]\n",
            "(A) [RESPONSE A]\n",
            "(B) [RESPONSE B]\n",
            "Assistant: Let’s think step-by-step: [CHAIN-OF-THOUGHT], index: 10, relevance_score: 0.37442517>\n",
            "\n",
            "--- Document 11 ---\n",
            "RerankResult<document['text']: from, those appearing in the PM and RL training data. Results are shown in Figure 3, where we compare\n",
            "SL-CAI models and RLHF models. The RLHF models include two types: (1) models trained on only helpfulness data, and (2) models trained on helpfulness and harmlessness. The ﬁgure also includes the RL-CAI (i.e.,\n",
            "RLAIF) models discussed in Section 4. A total of 10,274 helpfulness and 8,135 comparisons were collected\n",
            "for AB testing the 24 snapshots shown collectively in Figures 2 and 3.\n",
            "As expected from prior work, we ﬁnd that the helpful RLHF model is more helpful but also more harmful\n",
            "than HH RLHF. Furthermore, while SL-CAI is less helpful than both RL models, it is more harmless than the\n",
            "helpful RLHF model and more harmful than HH RLHF.8We also compare SL-CAI and pre-trained models\n",
            "in Figure 8, where the 52B-parameter SL-CAI model is shown as the initial snapshot of RL-CAI, while the\n",
            "7These principles were selected in an ad hoc manner for research purposes, and were not carefully designed as in\n",
            "[Glaese et al., 2022]. We have included these principles in Appendix C]\n",
            "8Note that the harmlessness Elo scores for the RLHF models look much closer to together compared to, index: 13, relevance_score: 0.3004689>\n",
            "\n",
            "--- Document 12 ---\n",
            "RerankResult<document['text']: Clark and Liane Lovitt to ﬁlter the PII.\n",
            "Model Training : Saurav Kadavath and Yuntao Bai trained the RLHF models we analyze. Yuntao Bai additionally trained the helpful and harmless preference models we use throughout the paper, and implemented\n",
            "the RS models as well. Kamal Ndousse and Andy Jones built the infrastructure used to train RLHF models. More generally, model pretraining was led by Sam McCandlish, Nicholas Joseph, Tom Brown, and\n",
            "Jared Kaplan. The majority of Anthropic’s technical staff contributed to the development of our efﬁcient distributed training infrastructure and the underlying machine learning systems. Core contributors include Tom\n",
            "Henighan, Scott Johnston, Sheer El Showk, Nicholas Joseph, Nelson Elhage, and Ben Mann. Scott Johnston\n",
            "and Sheer El-Showk in particular worked on optimizing pretraining for ML efﬁciency.\n",
            "Sampling : Efﬁcient sampling efforts were led by Tom Brown, and Tom Conerly carried out major aspects of\n",
            "the design, implementation and support for the system, with help from Zac Hatﬁeld Dodds.\n",
            "Cluster : Nova DasSarma and Eli Tran-Johnson managed the research cluster our research depended on and\n",
            "maintained its stability, making this research possible. Many others helped with these efforts, including Ben, index: 4, relevance_score: 0.29524565>\n",
            "\n",
            "--- Document 13 ---\n",
            "RerankResult<document['text']: and behavior. He helped to write the paper.\n",
            "Andy Jones and Kamal Ndoussse built the infrastructure for RL training of large language models. They also\n",
            "built associated plotting and monitoring systems and implemented the PPO algorithm. They helped with the\n",
            "design, implementation, and debugging of RLHF.\n",
            "Amanda Askell helped to design model evaluations, collected samples and evaluations from professional\n",
            "writers, built systems for improving the quality and quantity of data collection, and collaborated with Jared\n",
            "20In fact, this happened by accident when researchers ﬁne-tuned GPT-2 from human preferences with a sign-ﬂip bug.\n",
            "This resulted in a model which optimized for negative sentiment while preserving natural language [Ziegler et al., 2019].\n",
            "37\n",
            "and Jackson on associated evaluations. She also helped with the design and implementation of the human\n",
            "feedback interface. She helped to write the paper.\n",
            "Anna Chen helped with general RL and RLHF experimentation, and contributed to the research design.\n",
            "Nova DasSarma managed the underlying cluster infrastructure, making large scale RL training and human\n",
            "feedback collection possible.\n",
            "Dawn Drain trained the underlying code models and collaborated with Saurav on coding evaluations.\n",
            "Stanislav Fort performed the OOD detection and outlier exposure research and analysis on helpful versus\n",
            "harmful data samples., index: 20, relevance_score: 0.2739911>\n",
            "\n",
            "--- Document 14 ---\n",
            "RerankResult<document['text']: found that many RLHF policies were very frequently reproducing the same exaggerated responses to all\n",
            "remotely sensitive questions (e.g. recommending users seek therapy and professional help whenever they\n",
            "express any level of displeasure at all). This greatly limited these models’ utility. We still see a vestige of this\n",
            "19\n",
            "behavior in some of the examples provided in Section 6.2. We now believe these policies were the result of\n",
            "over-optimizing for harmlessness, while under-optimizing helpfulness.\n",
            "With our data collection procedure, we think this is quite intuitive. In order to get a very good score on\n",
            "red-teaming prompts, it’s probably sufﬁcient for models to respond with something like “I can’t answer that.”\n",
            "This does not require much sophistication (it just requires learning to classify harmful requests), and so we\n",
            "expect it is easier to learn than helpfulness.\n",
            "In Figure 14 (right), we show the policy’s PM score throughout training, after separating helpfulness and\n",
            "harmlessness prompts. On the left side of the same ﬁgure, we show the score distribution of PM comparison\n",
            "data, again separating helpful and harmless datasets. We observe that the policy’s harmlessness score is\n",
            "somewhat off-distribution, as it is on the upper tail of the harmlessness comparison data. On the other hand,, index: 16, relevance_score: 0.27224988>\n",
            "\n",
            "--- Document 15 ---\n",
            "RerankResult<document['text']: conditions. In the Q condition, the 175B model discriminates against Black students less with more RLHF\n",
            "steps, but fails to achieve demographic parity. In the Q+IF condition, the model achieves demographic parity\n",
            "at 600 RLHF steps. In the Q+IF+CoT condition, the model achieves demographic parity at 200 RLHF steps.\n",
            "In both conditions, further RLHF training causes the models to increasingly discriminate in favor of Black\n",
            "students.\n",
            "Fig. 5 (Right, A.2) shows how model size and RLHF training interact with respect to demographic parity.\n",
            "Across all experimental conditions, the amount of RLHF training has the greatest effect for models larger than\n",
            "22B parameters. Notably, for the 175B parameter model, at 50 steps of RLHF training, the Q+IF condition\n",
            "discriminates against Black students by 15% and at 1000 RLHF steps it discriminates in favor of Black\n",
            "students by 10%. For this benchmark, one can approximately achieve demographic parity by tuning both the\n",
            "model size and the amount of RLHF steps. But parity can only be achieved if models are instructed to not\n",
            "make decisions based on the race of the students.\n",
            "5 Discussion\n",
            "5.1 Conclusion\n",
            "We set out to test the hypothesis that large language models may have the capability to “morally selfcorrect”—to avoid producing harmful outputs—if instructed to do so in natural language. We ﬁnd strong, index: 17, relevance_score: 0.22078775>\n",
            "\n",
            "--- Document 16 ---\n",
            "RerankResult<document['text']: (RL). RL agents usually contain four key elements: a policy which defines the agent’s way of behaving at a given time; a reward signal which defines its goal; a value function which estimates the long-term value of different states of affairs; and a model of the environment which allows the agent to make predictions about how the environment will respond to its decisions (Sutton and Barto 2018, 6-7). RL agents then learn what to do by trying to maximise a numerical reward signal that they receive from their environment. They do this by engaging in exploration and revising their policies along the way. Sophisticated RL agents have proved particularly adept at game-playing, mastering the ancient Chinese board game of Go (which had, index: 15, relevance_score: 0.084681965>\n",
            "\n",
            "--- Document 17 ---\n",
            "RerankResult<document['text']: of learning from interaction with an environment and from numerical rewards (Sutton and Barto, 2018).\n",
            "RL methods have demonstrated success not only in game playing, for example checkers (Schaeﬀer et al.,\n",
            "1992), chess (Campbell et al., 2002; Silver et al., 2018), Go (Silver et al., 2016), poker (Moravčík et al.,\n",
            "2017), Atari(Mnihetal.,2015)andStarcraftII(Vinyalsetal.,2019), butalsoinreal-worldapplications,\n",
            "such as robotics (Kormushev et al., 2013), logistics (Refanidis et al., 2001), chemical synthesis (Segler\n",
            "et al., 2018) and personalised recommendations (Liu et al., 2019). In many of these applications, RL\n",
            "agents were able to achieve super-human performance, yet they can be prone to over-specialising to any\n",
            "single domain. In order to assess the performance of RL algorithms over a range of diﬀerent tasks, it is\n",
            "desirable to have platforms which expose diverse challenges through a uniﬁed interface. This approach\n",
            "was pioneered in the original Atari suite (Bellemare et al., 2013) and has been followed up by a variety\n",
            "of platforms, such as DeepMind Lab (Beattie et al., 2016), OpenAI Universe (OpenAI, 2016) and World, index: 8, relevance_score: 0.08042261>\n",
            "\n",
            "--- Document 18 ---\n",
            "RerankResult<document['text']: next token is sampled) in order to optimize some reward funct ion. Most of the existing work on RL and ALMs\n",
            "has focused on teaching LMs how to act rather than reason. The closest work on learning how to reason via\n",
            "RL is STaR ( Zelikman et al. ,2022), a bootstrapping-based approach that is discussed in Sect ion4.1\n",
            "RL is a natural framework for training LMs to act and use tools since many of these tools are nondiﬀerentiable (e.g. search engines, calculators or progra mming language interpreters). Additionally, many\n",
            "tasks that beneﬁt from interacting with tools resemble sequ ential decision making problems (e.g., navigating a web-browser to buy a speciﬁed product) and have a well-d eﬁned reward (e.g., 1 if the model buys\n",
            "the correct product and 0 otherwise). While there are early w orks focused on models that could interface\n",
            "with external tools, they employ ad-hoc tool-dependent arc hitectures ( Adolphs et al. ,2022;Buck et al. ,2018;\n",
            "16\n",
            "Nogueira and Cho ,2017;Zhong et al. ,2018). We do not cover them here since the main focus of our survey, index: 21, relevance_score: 0.07008091>\n",
            "\n",
            "--- Document 19 ---\n",
            "RerankResult<document['text']: the output is generated. These models often use a ﬁxed input and output vocabulary, which prevents\n",
            "them from learning representations for new words. One way to ﬁx this is to allow the decoder\n",
            "network to point back to some speciﬁc words or sub-sequences of the input and copy them onto the\n",
            "output sequence (Vinyals et al., 2015). Gulcehre et al. (2016) and Merity et al. (2017) combine this\n",
            "pointer mechanism with the original word generation layer in the decoder to allow the model to use\n",
            "either method at each decoding step.\n",
            "4.2 R EINFORCEMENT LEARNING FOR SEQUENCE GENERATION\n",
            "Reinforcement learning (RL) is a way of training an agent to interact with a given environment in\n",
            "order to maximize a reward. RL has been used to solve a wide variety of problems, usually when\n",
            "5\n",
            "an agent has to perform discrete actions before obtaining a reward, or when the metric to optimize\n",
            "is not differentiable and traditional supervised learning methods cannot be used. This is applicable\n",
            "to sequence generation tasks, because many of the metrics used to evaluate these tasks (like BLEU,\n",
            "ROUGE or METEOR) are not differentiable., index: 1, relevance_score: 0.059718315>\n",
            "\n",
            "--- Document 20 ---\n",
            "RerankResult<document['text']: unnecessary. More details about RL are provided in B.1.\n",
            "Throughout this paper we use rPM=the preference model score itself for the RL reward. Recall that as\n",
            "implied by equation (2.1), this means that the difference in rPMvalues between two samples AandBwill\n",
            "be related to the predicted probability P(A>B )thatAwill be preferred to Bvia\n",
            "P(A>B ) =1\n",
            "1 +erPM(B)\u0000rPM(A)(4.2)\n",
            "There is no good reason13to use this preference model score directly as the reward, but it has been used in\n",
            "prior work such as [Stiennon et al., 2020] and so for simplicity we will not explore variations on this choice\n",
            "here.\n",
            "In order to produce additional prompts (i.e. the human side of the conversations) for RLHF training, we used\n",
            "a large LM to generate them. For this purpose, we simply used few-shot learning, creating a context with\n",
            "about 10 existing high-quality human queries, and then sampling to generate more. We ﬁnd that the sample\n",
            "efﬁciency of RLHF is roughly the same on the original crowdworker-written prompt dataset and the modelgenerated one, so we combine the two for greater diversity during RLHF training. We used 137k prompts, index: 3, relevance_score: 0.049630567>\n",
            "\n",
            "--- Document 21 ---\n",
            "RerankResult<document['text']: Agarwal et al., 2019; Swamy et al., 2021).\n",
            "RL for Large Action Spaces. MIXER (Ranzato et al., 2016) combined ideas from schedule sampling\n",
            "and REINFORCE (Williams, 1992). Bahdanau et al. (2016) proposed an actor-critic algorithm to\n",
            "address the variance/large action space problems when using REINFORCE for language generation;\n",
            "follow-up works such as KG-A2C (Ammanabrolu & Hausknecht, 2020), TrufLL (Martin et al., 2022),\n",
            "AE-DQN (Zahavy et al., 2018), and GALAD (Ammanabrolu et al., 2022) addressed similar issues by\n",
            "attempting to eliminate and reduce the action space during exploration.\n",
            "RL for NLP. RL, often in the form of bandit learning, has been used to improve models in machine\n",
            "translation (Wu et al., 2016; Nguyen et al., 2017; Kiegeland & Kreutzer, 2021), summarization\n",
            "(Stiennon et al., 2020; Paulus et al., 2017), dialogue (Li et al., 2016; Zhou et al., 2017; Jaques et al.,\n",
            "2020), image captioning (Rennie et al., 2017), question generation (Pang & He, 2021), text-games, index: 19, relevance_score: 0.044535294>\n",
            "\n",
            "--- Document 22 ---\n",
            "RerankResult<document['text']: He ran most of our model comparison experiments.\n",
            "Tom Conerly helped with engineering, speciﬁcally with fast and efﬁcient sampling.\n",
            "Sheer El-Showk helped with pretraining research and dataset construction.\n",
            "Nelson Elhage contributed signiﬁcantly to pretraining and to engineering vision.\n",
            "Zac Hatﬁeld-Dodds helped with codebase maintenance and with engineering, speciﬁcally with fast and efﬁcient sampling.\n",
            "Danny Hernandez contributed to pretraining and especially to dataset design.\n",
            "Tristan Hume helped with streamlining our infrastructure.\n",
            "Scott Johnston helped with pretraining research.\n",
            "Shauna Kravec contributed to the development and use of our RL systems, and collaborated on RL research.\n",
            "Liane Lovitt helped with red-teaming, and in particular with designing the interface.\n",
            "Neel Nanda contributed to research discussions and priorities for alignment.\n",
            "Catherine Olsson helped advise on human feedback data collection, and contributed advice on alignment and\n",
            "evaluation.\n",
            "Dario Amodei advised the project and led efforts to build and test the RL infrastructure and ML.\n",
            "Tom Brown led engineering efforts, including efﬁcient pretraining, sampling, and the stability and design of\n",
            "RL systems.\n",
            "Jack Clark led societal impacts efforts and advised the project, including on various evaluations., index: 7, relevance_score: 0.03980655>\n",
            "\n",
            "--- Document 23 ---\n",
            "RerankResult<document['text']: appropriate processes in place.\n",
            "•Undue trust being placed in the system, especiallyasitcommunicateswithhumansinnaturallanguage, andcouldeasilybemistakenfor\n",
            "a human (Proudfoot, 2011; Watson, 2019).•The risk of job loss as a result of the automation of roles requiring language abilities (Frey\n",
            "and Osborne, 2017).\n",
            "4. Misspeciﬁcation\n",
            "FollowingKrakovnaetal.(2020b),weconsiderthe\n",
            "role of the designer of an AI system to be giving a\n",
            "speciﬁcation , understood quite broadly to encompass many aspects of the AI development process.\n",
            "For example, for an RL system, the speciﬁcation\n",
            "includes providing an environment in which the\n",
            "RL agent acts, a reward function that calculates\n",
            "reward signals, and a training algorithm for how\n",
            "the RL agent learns.\n",
            "Undesired behaviour can occur due to misspeciﬁcation– a mistake made by the designer in implementing the task speciﬁcation. In the language\n",
            "of Ortega and Maini (2018), the misspeciﬁcation\n",
            "is due to the gap between the ideal speciﬁcation\n",
            "(what the designer intended) and the design speciﬁcation (what the designer actually implements)., index: 14, relevance_score: 0.038658164>\n",
            "\n",
            "--- Document 24 ---\n",
            "RerankResult<document['text']: Tom Brown and Jared Kaplan, and much of Anthropic’s technical staff contributed to the development of our\n",
            "efﬁcient distributed training infrastructure and the underlying machine learning systems. Core contributors\n",
            "17\n",
            "include Tom Henighan, Scott Johnston, Sheer El Showk, Nelson Elhage, and Ben Mann. Scott Johnston\n",
            "in particular worked on optimizing pretraining for ML efﬁciency, while Sheer El Showk, Carol Chen, and\n",
            "Jennifer Zhou worked on data.\n",
            "Reinforcement Learning: The core RL infrastructure was built by Andy Jones and Kamal Ndousse in\n",
            "collaboration with Shauna Kravec and Dawn Drain. Development of the RL infrastructure has been led by\n",
            "Sam McCandlish and Dario Amodei.\n",
            "Sampling and Evaluation: Efﬁcient sampling efforts were led by Tom Brown, and Tom Conerly carried\n",
            "out major aspects of the design, implementation and support for the system, with help from Zac HatﬁeldDodds. Many members of Anthropic worked on our framework for evaluations, including Saurav Kadavath,\n",
            "Nicholas Schiefer, Nick Joseph, Tom Henighan, Amanda Askell, Jared Kaplan, Andy Jones, Ethan Perez,\n",
            "Scott Johnston, and Sam McCandlish. Jackson Kernion helped support human feedback data collection., index: 2, relevance_score: 0.029426433>\n",
            "\n",
            "--- Document 25 ---\n",
            "RerankResult<document['text']: token from our vocabulary V. The transition function P:S\u0002A! \u0001(S)deterministically appends\n",
            "an actionatto the end of the state st\u00001= (x0;\u0001\u0001\u0001;xm;a0;\u0001\u0001\u0001;at\u00001). This continues until the end\n",
            "of the horizon t\u0014Tand we obtain a state sT= (x0;\u0001\u0001\u0001;xm;a0;\u0001\u0001\u0001;aT). At the end of an episode\n",
            "a rewardR:S\u0002A\u0002Y! R1that depends on the ( sT;y) (e.g., an automated metric like PARENT\n",
            "Dhingra et al. (2019)) is emitted. RL4LMs provides an OpenAI gym (Brockman et al., 2016) style\n",
            "3\n",
            "Published as a conference paper at ICLR 2023\n",
            "API for an RL environment that simulates this LM-Based MDP formulation. This abstraction allows\n",
            "for new tasks to be added quickly with compatibility across all implemented algorithms.\n",
            "3.2 R EWARD FUNCTIONS AND EVALUATION METRICS\n",
            "Because RL4LMs provides a generic interface for per-token or per-sequence generation rewards, it\n",
            "is possible to quickly apply a wide array of RL algorithms to a similarly diverse range of textual, index: 6, relevance_score: 0.011280472>\n"
          ]
        }
      ],
      "source": [
        "rerank_docs[0].document[\"text\"]\n",
        "for i, rerank_docs in enumerate(rerank_docs, 1):\n",
        "    print(f\"\\n--- Document {i} ---\\n{rerank_docs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOUw6AVFDhG9"
      },
      "source": [
        "The reordered results look like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-lOqKxhIy4C",
        "outputId": "4f7cf0ff-9a76-461f-d1cd-d2d1730390ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'text': 'preferences and values which are diﬃcult to capture by hard- coded reward functions.\\nRLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,\\nranking two model generations for the same prompt. This data is then collected to learn a reward model\\nthat predicts a scalar reward given any generated text. The r eward captures human preferences when\\njudging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient\\nalgorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM\\npre-trained via self-supervised learning. However, for mo re complex tasks, the model’s generations may not\\nbe good enough. In such cases, RLHF is typically applied afte r an initial supervised ﬁne-tuning phase using\\na small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;\\nOuyang et al. ,2022;Stiennon et al. ,2020).\\nA successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.\\n(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing'},\n",
              " {'text': 'the output is generated. These models often use a ﬁxed input and output vocabulary, which prevents\\nthem from learning representations for new words. One way to ﬁx this is to allow the decoder\\nnetwork to point back to some speciﬁc words or sub-sequences of the input and copy them onto the\\noutput sequence (Vinyals et al., 2015). Gulcehre et al. (2016) and Merity et al. (2017) combine this\\npointer mechanism with the original word generation layer in the decoder to allow the model to use\\neither method at each decoding step.\\n4.2 R EINFORCEMENT LEARNING FOR SEQUENCE GENERATION\\nReinforcement learning (RL) is a way of training an agent to interact with a given environment in\\norder to maximize a reward. RL has been used to solve a wide variety of problems, usually when\\n5\\nan agent has to perform discrete actions before obtaining a reward, or when the metric to optimize\\nis not differentiable and traditional supervised learning methods cannot be used. This is applicable\\nto sequence generation tasks, because many of the metrics used to evaluate these tasks (like BLEU,\\nROUGE or METEOR) are not differentiable.'},\n",
              " {'text': 'unnecessary. More details about RL are provided in B.1.\\nThroughout this paper we use rPM=the preference model score itself for the RL reward. Recall that as\\nimplied by equation (2.1), this means that the difference in rPMvalues between two samples AandBwill\\nbe related to the predicted probability P(A>B )thatAwill be preferred to Bvia\\nP(A>B ) =1\\n1 +erPM(B)\\x00rPM(A)(4.2)\\nThere is no good reason13to use this preference model score directly as the reward, but it has been used in\\nprior work such as [Stiennon et al., 2020] and so for simplicity we will not explore variations on this choice\\nhere.\\nIn order to produce additional prompts (i.e. the human side of the conversations) for RLHF training, we used\\na large LM to generate them. For this purpose, we simply used few-shot learning, creating a context with\\nabout 10 existing high-quality human queries, and then sampling to generate more. We ﬁnd that the sample\\nefﬁciency of RLHF is roughly the same on the original crowdworker-written prompt dataset and the modelgenerated one, so we combine the two for greater diversity during RLHF training. We used 137k prompts'},\n",
              " {'text': 'Tom Brown and Jared Kaplan, and much of Anthropic’s technical staff contributed to the development of our\\nefﬁcient distributed training infrastructure and the underlying machine learning systems. Core contributors\\n17\\ninclude Tom Henighan, Scott Johnston, Sheer El Showk, Nelson Elhage, and Ben Mann. Scott Johnston\\nin particular worked on optimizing pretraining for ML efﬁciency, while Sheer El Showk, Carol Chen, and\\nJennifer Zhou worked on data.\\nReinforcement Learning: The core RL infrastructure was built by Andy Jones and Kamal Ndousse in\\ncollaboration with Shauna Kravec and Dawn Drain. Development of the RL infrastructure has been led by\\nSam McCandlish and Dario Amodei.\\nSampling and Evaluation: Efﬁcient sampling efforts were led by Tom Brown, and Tom Conerly carried\\nout major aspects of the design, implementation and support for the system, with help from Zac HatﬁeldDodds. Many members of Anthropic worked on our framework for evaluations, including Saurav Kadavath,\\nNicholas Schiefer, Nick Joseph, Tom Henighan, Amanda Askell, Jared Kaplan, Andy Jones, Ethan Perez,\\nScott Johnston, and Sam McCandlish. Jackson Kernion helped support human feedback data collection.'}]"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the reranked texts\n",
        "[res.document for res in rerank_docs.results]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKiUwIikMGU1"
      },
      "source": [
        "Let's write a function to allow us to more easily compare the original results vs. reranked results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfFFNLu2MLrt"
      },
      "outputs": [],
      "source": [
        "def compare(query: str, k: int = 25, top_n: int = 3):\n",
        "    # Step 1: Retrieve from FAISS\n",
        "    original_docs = retrieve(query, k=k)\n",
        "\n",
        "    # Step 2: Rerank using Cohere\n",
        "    rerank_results = co.rerank(\n",
        "        query=query,\n",
        "        documents=original_docs,\n",
        "        top_n=top_n,\n",
        "        model=\"rerank-v3.5\"\n",
        "    ).results\n",
        "\n",
        "    # Step 3: Extract reranked text\n",
        "    reranked_texts = [res.document[\"text\"] for res in rerank_results]\n",
        "\n",
        "    print(f\"\\n Query: {query}\\n\")\n",
        "    \n",
        "    print(f\"--- Top {top_n} FAISS Results (Before Rerank) ---\")\n",
        "    for i, text in enumerate(original_docs[:top_n]):\n",
        "        clean_text = text[:100].replace(\"\\n\", \" \")\n",
        "        print(f\"{i+1:02d}: {clean_text}...\")\n",
        "\n",
        "    print(f\"\\n--- Top {top_n} Reranked Results (After Rerank) ---\")\n",
        "    for i, text in enumerate(reranked_texts):\n",
        "        clean_text = text[:100].replace(\"\\n\", \" \")\n",
        "        print(f\"{i+1:02d}: {clean_text}...\")\n",
        "\n",
        "    print(f\"\\n Rank Changes (FAISS → Rerank):\")\n",
        "    rank_map = {}\n",
        "    for i, text in enumerate(reranked_texts):\n",
        "        try:\n",
        "            original_pos = original_docs.index(text)\n",
        "            rank_map[i+1] = original_pos + 1\n",
        "            print(f\"Reranked #{i+1:02d} was FAISS #{original_pos+1:02d}\")\n",
        "        except ValueError:\n",
        "            print(f\"Reranked #{i+1:02d} not found in FAISS top-{k} results.\")\n",
        "            rank_map[i+1] = None\n",
        "\n",
        "    return rank_map\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gThAy0k4bOo"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔎 Query: can you explain why we would want to do rlhf?\n",
            "\n",
            "--- Top 3 FAISS Results (Before Rerank) ---\n",
            "01: preferences and values which are diﬃcult to capture by hard- coded reward functions. RLHF works by u...\n",
            "02: the output is generated. These models often use a ﬁxed input and output vocabulary, which prevents t...\n",
            "03: Tom Brown and Jared Kaplan, and much of Anthropic’s technical staff contributed to the development o...\n",
            "\n",
            "--- Top 3 Reranked Results (After Rerank) ---\n",
            "01: model to estimate the eventual performance of a larger RL policy. The slopes of these lines also exp...\n",
            "02: preferences and values which are diﬃcult to capture by hard- coded reward functions. RLHF works by u...\n",
            "03: by being evasive [4]. Our second contribution is to release our dataset of 38,961 red team attacks f...\n",
            "\n",
            "🔁 Rank Changes (FAISS → Rerank):\n",
            "Reranked #01 was FAISS #10\n",
            "Reranked #02 was FAISS #01\n",
            "Reranked #03 was FAISS #19\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{1: 10, 2: 1, 3: 19}"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compare(query, 25, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "groq_api_key = os.getenv(\"groq_api\")\n",
        "\n",
        "def generate_answer(question: str, contexts: list[str]) -> str:\n",
        "    \"\"\"\n",
        "    Generates an answer to the user's question using the provided contexts\n",
        "    and a Groq-hosted LLM via LangChain Expression Language (LCEL).\n",
        "\n",
        "    Args:\n",
        "        question (str): The user's question.\n",
        "        contexts (list[str]): A list of relevant document contexts.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer from the LLM.\n",
        "    \"\"\"\n",
        "    # 1. Initialize the Groq Chat model\n",
        "    llm = ChatGroq(\n",
        "        model_name='gemma2-9b-it',\n",
        "        temperature=0, # Keep temperature at 0 for more factual/less creative answers\n",
        "        groq_api_key=groq_api_key\n",
        "    )\n",
        "\n",
        "    # 2. Define the RAG prompt template\n",
        "    prompt_template = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \n",
        "         \"Answer the user question **only** with facts found in the context. \"\n",
        "         \"If the answer is not in the context, state that you cannot answer from the provided information.\\n\\n\"\n",
        "         \"Context:\\n{context}\"), # 'context' is the variable where retrieved docs will be injected\n",
        "        (\"user\", \"{question}\")\n",
        "    ])\n",
        "\n",
        "    # This creates a chain that takes 'context' and 'question' as input,\n",
        "    # formats them into the prompt, and sends to the LLM.\n",
        "    # Note: `create_stuff_documents_chain` is more for LangChain's Document objects.\n",
        "    # We are directly formatting the context string in the LCEL chain below.\n",
        "    # For a simple RAG chain:\n",
        "    generation_chain = (\n",
        "        {\n",
        "            \"context\": lambda x: \"\\n\".join(f\"- {c}\" for c in x[\"contexts\"]), # Format contexts from list of strings\n",
        "            \"question\": RunnablePassthrough() # Pass the question through\n",
        "        }\n",
        "        | prompt_template\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Invoke the chain with the question and contexts\n",
        "        result = generation_chain.invoke({\"question\": question, \"contexts\": contexts})\n",
        "        return result.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating answer with Groq/LangChain: {e}\")\n",
        "        return \"Error: Could not generate answer.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error generating answer with Groq/LangChain: Connection error.\n",
            "Error: Could not generate answer.\n"
          ]
        }
      ],
      "source": [
        "reranked_context_texts = []\n",
        "for hit in rerank_docs.results:\n",
        "    # Use hit.index to get the actual document content from the original list\n",
        "    original_doc_text = docs[hit.index]\n",
        "    reranked_context_texts.append(original_doc_text)\n",
        "\n",
        "# Select only the top 2 reranked documents for the LLM's context\n",
        "# This is where your 'top_context' (named 'final_contexts_for_llm' in the Canvas) is created\n",
        "final_contexts_for_llm = reranked_context_texts[:25]\n",
        "\n",
        "# Now, you can pass these top 2 contexts to your generation function:\n",
        "answer = generate_answer(query, final_contexts_for_llm)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This document does not contain the answer to why we would want to do rlhf.\n"
          ]
        }
      ],
      "source": [
        "final_contexts_for_llm = doc[:25]\n",
        "\n",
        "# Now, you can pass these top 2 contexts to your generation function:\n",
        "answer = generate_answer(query, final_contexts_for_llm)\n",
        "print(answer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RAG",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
