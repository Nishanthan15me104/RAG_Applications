{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528582a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nis44\\AppData\\Local\\Temp\\ipykernel_15012\\95210927.py:30: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
      "c:\\Users\\nis44\\anaconda3\\RAG\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\nis44\\anaconda3\\RAG\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 12:29:43.116000 15012 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model (all-MiniLM-L6-v2) initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"groq_api\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. Initialize Models and Components ---\n",
    "\n",
    "# LLM for generating hypothetical documents (e.g., OpenAI's gpt-3.5-turbo)\n",
    "# You can use other LLMs like Groq's ChatGroq if you prefer, with appropriate imports\n",
    "hyde_llm = ChatGroq(\n",
    "        model_name='gemma2-9b-it',\n",
    "        temperature=0, # Keep temperature at 0 for more factual/less creative answers\n",
    "        groq_api_key=groq_api_key\n",
    "    )\n",
    "\n",
    "# Embedding model for both hypothetical documents and real documents\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(\"Embedding model (all-MiniLM-L6-v2) initialized.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9626e0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating a dummy FAISS vector store with sample documents...\n",
      "Dummy FAISS vector store created with 10 documents.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Create a Dummy Vector Store (If you don't have one saved) ---\n",
    "# In a real scenario, your 'docs' would come from loaded data and your 'vectorstore'\n",
    "# would be built from them. For this example, we'll create a small one on the fly.\n",
    "\n",
    "# Sample content for our \"knowledge base\"\n",
    "raw_documents = [\n",
    "    \"Reinforcement Learning from Human Feedback (RLHF) is a technique that fine-tunes language models.\",\n",
    "    \"It uses human preferences to train a reward model, which then guides the LLM.\",\n",
    "    \"The primary goal of RLHF is to align AI behavior with human values, making models more helpful and harmless.\",\n",
    "    \"Without RLHF, LLMs might generate undesirable outputs like toxic or biased content.\",\n",
    "    \"HyDE stands for Hypothetical Document Embedding, and it improves retrieval by generating a sample document.\",\n",
    "    \"Step-Back Prompting enhances LLM reasoning by making the model derive high-level concepts first.\",\n",
    "    \"A Reflection Agent allows an LLM to self-critique and refine its own answers iteratively.\",\n",
    "    \"FAISS is a library for efficient similarity search and clustering of dense vectors.\",\n",
    "    \"LangChain provides frameworks for building applications with LLMs, including RAG and agents.\",\n",
    "    \"Groq offers very fast inference for LLMs like Llama3.\"\n",
    "]\n",
    "\n",
    "# Create embeddings for our dummy documents and build a FAISS index in memory\n",
    "print(\"\\nCreating a dummy FAISS vector store with sample documents...\")\n",
    "# Ensure docs are processed into LangChain Document objects if not already\n",
    "from langchain_core.documents import Document\n",
    "documents_for_faiss = [Document(page_content=d) for d in raw_documents]\n",
    "vectorstore = FAISS.from_documents(documents_for_faiss, embedding_model)\n",
    "docs = [doc.page_content for doc in documents_for_faiss] # Original texts list\n",
    "print(f\"Dummy FAISS vector store created with {len(docs)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0da93929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyDE Prompt Template defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Define the HyDE Prompt Template ---\n",
    "# This prompt instructs the LLM to generate a hypothetical document.\n",
    "hyde_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"Please write a short, relevant, and well-written hypothetical document \"\n",
    "     \"that could answer the user's question. This document should be detailed \"\n",
    "     \"and sound like a real piece of text from a knowledge base.\\n\\n\"\n",
    "     \"Hypothetical Document:\"), # We want the LLM to fill this\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "print(\"HyDE Prompt Template defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a80c9045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyDE Chain (LLM for generation) defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Create the HyDE Chain ---\n",
    "# This chain takes a question, generates a hypothetical document, and outputs it as a string.\n",
    "hyde_chain = hyde_prompt | hyde_llm | StrOutputParser()\n",
    "print(\"HyDE Chain (LLM for generation) defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "850aa447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. The HyDE Retrieval Function ---\n",
    "def hyde_retrieve(query: str, k: int = 3) -> list[str]:\n",
    "    \"\"\"\n",
    "    Performs retrieval using the Hypothetical Document Embedding (HyDE) technique.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's original short query.\n",
    "        k (int): The number of top relevant real documents to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of the actual retrieved document texts.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- HyDE Retrieval for query: '{query}' ---\")\n",
    "    \n",
    "    # Step A: Generate the hypothetical document\n",
    "    print(\"Generating hypothetical document...\")\n",
    "    hypothetical_document = hyde_chain.invoke({\"question\": query})\n",
    "    print(f\"Hypothetical Document Generated:\\n---\\n{hypothetical_document[:200]}...\\n---\")\n",
    "\n",
    "    # Step B: Embed the hypothetical document\n",
    "    print(\"Embedding hypothetical document...\")\n",
    "    # embed_documents expects a list, even if it's just one document\n",
    "    hypothetical_embedding = embedding_model.embed_documents([hypothetical_document])[0] # [0] to get the single vector\n",
    "    hypothetical_embedding_np = np.array(hypothetical_embedding).astype('float32').reshape(1, -1)\n",
    "    \n",
    "    # Optional: Normalize the embedding if your FAISS index expects it\n",
    "    # faiss.normalize_L2(hypothetical_embedding_np)\n",
    "    \n",
    "    # Step C: Use the hypothetical embedding to search the real vector store\n",
    "    print(f\"Searching FAISS with hypothetical embedding for top {k} documents...\")\n",
    "    distances, indices = vectorstore.index.search(hypothetical_embedding_np, k)\n",
    "    \n",
    "    retrieved_docs_content = []\n",
    "    # vectorstore.index_to_docstore_id and vectorstore.docstore._dict are internal\n",
    "    # properties of LangChain's FAISS class for accessing original documents.\n",
    "    for i in indices[0]: # indices[0] contains the actual indices\n",
    "        if i >= 0 and i < len(docs): # Ensure index is valid\n",
    "            retrieved_docs_content.append(docs[i])\n",
    "    \n",
    "    return retrieved_docs_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bacda813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- HyDE Retrieval for query: 'What is RLHF and why is it used?' ---\n",
      "Generating hypothetical document...\n",
      "Hypothetical Document Generated:\n",
      "---\n",
      "## RLHF: Reinforcement Learning from Human Feedback\n",
      "\n",
      "**Definition:**\n",
      "\n",
      "Reinforcement Learning from Human Feedback (RLHF) is a technique used to train artificial intelligence (AI) models, particularly l...\n",
      "---\n",
      "Embedding hypothetical document...\n",
      "Searching FAISS with hypothetical embedding for top 4 documents...\n",
      "\n",
      "--- Top 4 Retrieved Documents using HyDE ---\n",
      "Document 1: Reinforcement Learning from Human Feedback (RLHF) is a technique that fine-tunes language models....\n",
      "Document 2: It uses human preferences to train a reward model, which then guides the LLM....\n",
      "Document 3: The primary goal of RLHF is to align AI behavior with human values, making models more helpful and harmless....\n",
      "Document 4: Step-Back Prompting enhances LLM reasoning by making the model derive high-level concepts first....\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Example Usage ---\n",
    "user_query = \"What is RLHF and why is it used?\"\n",
    "top_n_retrieved = 4 # How many real documents we want after HyDE retrieval\n",
    "\n",
    "retrieved_contexts = hyde_retrieve(user_query, k=top_n_retrieved)\n",
    "\n",
    "print(f\"\\n--- Top {top_n_retrieved} Retrieved Documents using HyDE ---\")\n",
    "if retrieved_contexts:\n",
    "    for i, doc_content in enumerate(retrieved_contexts):\n",
    "        cleaned_doc_content = doc_content.replace('\\n', ' ').strip()\n",
    "        print(f\"Document {i+1}: {cleaned_doc_content[:200]}...\") # Print first 200 chars for brevity\n",
    "else:\n",
    "    print(\"No documents retrieved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
